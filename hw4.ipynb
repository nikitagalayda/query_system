{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics(tree):\n",
    "    topics = []\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    for child in root:\n",
    "        topics.append(child)\n",
    "    \n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_line_length(f):\n",
    "    with open(f) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: topic, max number of keywords\n",
    "# Output: set of strings of keywords\n",
    "\n",
    "def get_topic_keywords_set(topic, n=5):\n",
    "    f = ['\\n', '、', '。', '，']\n",
    "    keywords_dict = {}\n",
    "    keywords = list(topic[1].text) + list(topic[2].text) + list(topic[3].text) + list(topic[4].text)\n",
    "    keywords = [x for x in keywords if x not in f]\n",
    "\n",
    "    for k in keywords:\n",
    "        if str(k) in keywords_dict:\n",
    "            keywords_dict[str(k)] += 1\n",
    "        else:\n",
    "            keywords_dict[str(k)] = 1\n",
    "        \n",
    "    res = sorted(keywords_dict.items(), key=lambda item: item[1])\n",
    "    res = res[len(res)-n:]\n",
    "    res = [x[0] for x in res]\n",
    "\n",
    "    return res\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: set of keyword strings\n",
    "# Output: set of keyword_id ints\n",
    "\n",
    "def get_keywords_id_set(keywords, model_dir):\n",
    "    kw_set = []\n",
    "    f = open('.{}/vocab.all'.format(model_dir), 'r', encoding='utf-8')\n",
    "    \n",
    "    for l, w in enumerate(f.readlines()):\n",
    "        w = w.strip()\n",
    "        if str(w) in keywords:\n",
    "            kw_set.append(l)\n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "    return kw_set\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: keyword id\n",
    "# Output: a tuple of (tf, set of doc_id strings)\n",
    "\n",
    "def get_kw_docs_set(kw_id, model_dir):\n",
    "    kw_id = str(kw_id)\n",
    "    doc_set = set()\n",
    "    f = open('.{}/inverted-file'.format(model_dir), 'r', encoding='utf-8')\n",
    "    found = False\n",
    "    a = -1\n",
    "    tf = 0\n",
    "    \n",
    "    for i, line in enumerate(f):\n",
    "        if found and i > a:\n",
    "            found = False\n",
    "            a = -1\n",
    "        l = list(str(line).strip().split(' '))\n",
    "        # word id metadata\n",
    "        if len(l) == 3:\n",
    "            if l[0] == kw_id or l[1] == kw_id:\n",
    "                found = True\n",
    "#             if l[1] == '-1':\n",
    "                a = i+int(l[2])\n",
    "                tf += int(l[2])\n",
    "        elif len(l) == 2 and a != -1 and found:\n",
    "            doc_set.add(l[0])\n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "    return (tf, doc_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: keyword id\n",
    "# Output: a tuple of (tf, set of doc_id strings)\n",
    "\n",
    "def get_wid_dict(model_dir):\n",
    "    f = open('.{}/inverted-file'.format(model_dir), 'r', encoding='utf-8')\n",
    "    res = {}\n",
    "    curr = None\n",
    "    \n",
    "    for i, line in enumerate(f):\n",
    "        \n",
    "        l = list(str(line).strip().split(' '))\n",
    "        \n",
    "        if len(l) == 3:\n",
    "            curr = int(l[0])\n",
    "            if curr not in res:\n",
    "                res[curr] = {}\n",
    "        elif len(l) == 2:\n",
    "            if int(l[0]) in res[curr]:\n",
    "                res[curr][int(l[0])] += int(l[1])\n",
    "            else:\n",
    "                res[curr][int(l[0])] = int(l[1])\n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: set of strings of keyword ids\n",
    "# Output: list of sets of doc_id with keyword\n",
    "\n",
    "def get_topic_kw_doc_set(topic_kw_set, model_dir):\n",
    "    res = []\n",
    "    \n",
    "    for kw_id in topic_kw_set:\n",
    "        print(kw_id)\n",
    "        res.append(get_kw_docs_set(kw_id, model_dir))\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics_tf_docs(topics_kw_sets, model_dir):\n",
    "    res = []\n",
    "    \n",
    "    for t in topics_kw_sets:\n",
    "        res.append(get_topic_kw_doc_set(t, model_dir))\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: topics\n",
    "# Output: list of sets of topic keywords\n",
    "\n",
    "def get_topics_keywords(topics):\n",
    "    topics_keywords = []\n",
    "\n",
    "    for t in topics:\n",
    "        kw = get_topic_keywords_set(t)\n",
    "        topics_keywords.append(kw)\n",
    "        \n",
    "    return topics_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: \n",
    "# Output: list of sets of keyword ids for each topic\n",
    "\n",
    "def get_topics_kw_wordid_sets(topics_keywords, model_dir):\n",
    "    topics_kw_sets = []\n",
    "\n",
    "    for kw in topics_keywords:\n",
    "        topics_kw_sets.append(get_keywords_id_set(kw, model_dir))\n",
    "        \n",
    "    return topics_kw_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_vector_dict(keyword_ids, t_dict, model_dir):\n",
    "    f = open('.{}/file-list'.format(model_dir), 'r', encoding='utf-8')\n",
    "    res = {}\n",
    "    \n",
    "    for i, line in enumerate(f):\n",
    "        # find tf : number of times this word occurs in the document i\n",
    "        v = []\n",
    "        for kw_id in list(keyword_ids):\n",
    "            if i in t_dict[kw_id]:\n",
    "                v.append(t_dict[kw_id][i])\n",
    "            else:\n",
    "                v.append(0)\n",
    "    \n",
    "        res[i] = norm_vec(v)\n",
    "        \n",
    "    return res\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_vectors_all_topics(keyword_id_set, t_dict):\n",
    "    res = []\n",
    "    \n",
    "    for s in keyword_id_set:\n",
    "        res.append(get_doc_vector_dict(s, t_dict, model_dir))\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_vec(vec):\n",
    "    res = []\n",
    "    m = max(vec)\n",
    "    \n",
    "    for v in vec:\n",
    "        if m != 0:\n",
    "            res.append(float(v)/m)\n",
    "        else:\n",
    "            res.append(0.0)\n",
    "            \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_vectors(kw_lists, idf_dict):\n",
    "    res = []\n",
    "    \n",
    "    for l in kw_lists:\n",
    "        v = []\n",
    "        for kw in l:\n",
    "            v.append(idf_dict[kw])\n",
    "        res.append(v)\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(qv, dv):\n",
    "    res = []\n",
    "    \n",
    "    for i in range(len(qv)):\n",
    "        tmp = {}\n",
    "        for k, v in dv[i].items():\n",
    "            tmp[k] = np.dot(qv[i], np.array(v))\n",
    "        res.append(tmp)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_docs(score_vectors, n=5):\n",
    "    res = []\n",
    "    \n",
    "    for s in score_vectors:\n",
    "        res.append(dict(sorted(s.items(), key=lambda item: item[1], reverse=True)[:n]))\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_name(doc_id, model_dir):\n",
    "    f = '.{}/file-list'.format(model_dir)\n",
    "    with open(f) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            if i == int(doc_id):\n",
    "                s = str(l).strip()\n",
    "                s = s.split('/')[3].lower()\n",
    "                return s\n",
    "    return 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prediction(d, model_dir, ranked_list_name):\n",
    "    res = {}\n",
    "    \n",
    "    for i in range(len(d)):\n",
    "        tmp = []\n",
    "        for k, v in d[i].items():\n",
    "            tmp.append(get_doc_name(k, model_dir))\n",
    "        new_key = '0'+'{}'.format(i+11)\n",
    "        res[new_key] = \" \".join(tmp)\n",
    "    \n",
    "    res = pd.DataFrame.from_dict(res, orient='index', columns=['retrieved_docs'])\n",
    "    res.index.name = 'query_id'\n",
    "    \n",
    "    res.to_csv('./{}.csv'.format(ranked_list_name))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_N(model_dir):\n",
    "    f = '.{}/file-list'.format(model_dir)\n",
    "    with open(f) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idf(df, N):\n",
    "    return np.log(N/df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute(t_dict, r, query_file, ranked_list_name, model_dir, NTCIR_dir):\n",
    "    cf_dict = dict([(x, sum(y.values())) for x, y in t_dict.items()])\n",
    "    df_dict = dict([(x, len(y.values())) for x, y in t_dict.items()])\n",
    "    N = get_N(model_dir)\n",
    "    idf_dict = dict([(x, get_idf(df_dict[x], N)) for x, y in t_dict.items()])\n",
    "    \n",
    "    tree = ET.parse('.'+query_file)\n",
    "    topics = get_topics(tree)\n",
    "    \n",
    "    topics_keywords = get_topics_keywords(topics)\n",
    "    topics_kw_sets = get_topics_kw_wordid_sets(topics_keywords, model_dir)\n",
    "    \n",
    "    query_vectors = get_query_vectors(topics_kw_sets, idf_dict)\n",
    "    query_vectors = np.array(query_vectors)\n",
    "\n",
    "    doc_vectors = get_doc_vectors_all_topics(topics_kw_sets, t_dict, model_dir)\n",
    "    score_vectors = get_scores(query_vectors, doc_vectors)\n",
    "    rel_docs_scores = get_n_docs(score_vectors, 5)\n",
    "    \n",
    "    #     ROCCHIO FEEDBACK\n",
    "    if r:\n",
    "        Dr = get_dr(doc_vectors, rel_docs_scores)\n",
    "        Dnr = get_dnr(doc_vectors, rel_docs_scores)\n",
    "        query_vectors = rocchio_feedback(query_vectors, Dr, Dnr)\n",
    "        doc_vectors = get_doc_vectors_all_topics(topics_kw_sets, t_dict, model_dir)\n",
    "        score_vectors = get_scores(query_vectors, doc_vectors)\n",
    "        rel_docs_scores = get_n_docs(score_vectors, 5)\n",
    "    \n",
    "    create_prediction(rel_docs_scores, model_dir, ranked_list_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dr(doc_vectors, rel_docs_scores):\n",
    "    res = []\n",
    "    \n",
    "    for i in range(len(rel_docs_scores)):\n",
    "        tmp = []\n",
    "        for k, v in rel_docs_scores[i].items():\n",
    "            tmp.append(doc_vectors[i][k])\n",
    "        res.append(tmp)\n",
    "\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dnr(doc_vectors, rel_docs_scores):\n",
    "    d_c = doc_vectors.copy()\n",
    "    res = []\n",
    "    \n",
    "    for i in range(len(rel_docs_scores)):\n",
    "        tmp = []\n",
    "        for k, v in doc_vectors[i].items():\n",
    "            if k not in rel_docs_scores[i]: \n",
    "                tmp.append(doc_vectors[i][k])\n",
    "        res.append(tmp)\n",
    "\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_queries_v_sum(queries_vectors):\n",
    "    res = []\n",
    "    for qv in queries_vectors:\n",
    "        ar = np.array(qv)\n",
    "        s = np.add.reduce(ar)\n",
    "        res.append(s)\n",
    "        \n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rocchio_feedback(query_vectors, Dr, Dnr, a=1, b=0.75, g=0.15):\n",
    "    res = []\n",
    "    Dr_sum = get_queries_v_sum(Dr)\n",
    "    Dnr_sum = get_queries_v_sum(Dnr)\n",
    "    query_vectors = np.array(query_vectors)\n",
    "    \n",
    "    for i in range(len(query_vectors)):\n",
    "        qm = a*query_vectors[i] + (b/len(Dr[i]))*Dr_sum[i] - (g/len(Dnr[i]))*Dnr_sum[i]\n",
    "        res.append(qm)\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "metadata": {},
   "outputs": [],
   "source": [
    "argv = ['-r', '-i', '/queries/query-train.xml', '-o', 'ranked_list.csv', '-m', '/model', '-d', '/CIRB010']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = False\n",
    "query_file = None\n",
    "ranked_list_name = None\n",
    "model_dir = None\n",
    "NTCIR_dir = None\n",
    "\n",
    "for i in range(len(argv)-1):\n",
    "    c = argv[i]\n",
    "    \n",
    "    if c == '-r':\n",
    "        r = True\n",
    "    elif c == '-i':\n",
    "        query_file = argv[i+1]\n",
    "    elif c == '-o':\n",
    "        ranked_list_name = argv[i+1]\n",
    "    elif c == '-m':\n",
    "        model_dir = argv[i+1]\n",
    "    elif c == '-d':\n",
    "        NTCIR_dir = argv[i+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_id : (doc_id : occurrences)\n",
    "if False:\n",
    "    t_dict = get_wid_dict(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_doc_vector_dict() missing 1 required positional argument: 'model_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-789-18ed569e96ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mranked_list_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNTCIR_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-781-ba398e5b849c>\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(t_dict, r, query_file, ranked_list_name, model_dir, NTCIR_dir)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mquery_vectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery_vectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mdoc_vectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_doc_vectors_all_topics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopics_kw_sets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mscore_vectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery_vectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_vectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mrel_docs_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_n_docs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore_vectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-772-671b5cf913fd>\u001b[0m in \u001b[0;36mget_doc_vectors_all_topics\u001b[1;34m(keyword_id_set, t_dict)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkeyword_id_set\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_doc_vector_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: get_doc_vector_dict() missing 1 required positional argument: 'model_dir'"
     ]
    }
   ],
   "source": [
    "execute(t_dict, r, query_file, ranked_list_name, model_dir, NTCIR_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
