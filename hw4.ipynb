{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 928,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import sys\n",
    "import pathlib\n",
    "\n",
    "curr_dir = pathlib.Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 897,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics(tree):\n",
    "    topics = []\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    for child in root:\n",
    "        topics.append(child)\n",
    "    \n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_line_length(f):\n",
    "    with open(f) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 899,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: topic, max number of keywords\n",
    "# Output: set of strings of keywords\n",
    "\n",
    "def get_topic_keywords_set(topic, n=20):\n",
    "    f = ['\\n', '、', '。', '，']\n",
    "    keywords_dict = {}\n",
    "    keywords = list(topic[1].text) + list(topic[2].text) + list(topic[3].text) + list(topic[4].text)\n",
    "    keywords = [x for x in keywords if x not in f]\n",
    "\n",
    "    for k in keywords:\n",
    "        if str(k) in keywords_dict:\n",
    "            keywords_dict[str(k)] += 1\n",
    "        else:\n",
    "            keywords_dict[str(k)] = 1\n",
    "        \n",
    "    res = sorted(keywords_dict.items(), key=lambda item: item[1])\n",
    "    res = res[len(res)-n:]\n",
    "    res = [x[0] for x in res]\n",
    "\n",
    "    return res\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 900,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: set of keyword strings\n",
    "# Output: set of keyword_id ints\n",
    "\n",
    "def get_keywords_id_set(keywords, model_dir):\n",
    "    kw_set = []\n",
    "    f = open('{}/{}/vocab.all'.format(curr_dir, model_dir), 'r', encoding='utf-8')\n",
    "    \n",
    "    for l, w in enumerate(f.readlines()):\n",
    "        w = w.strip()\n",
    "        if str(w) in keywords:\n",
    "            kw_set.append(l)\n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "    return kw_set\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 901,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: keyword id\n",
    "# Output: a tuple of (tf, set of doc_id strings)\n",
    "\n",
    "def get_kw_docs_set(kw_id, model_dir):\n",
    "    kw_id = str(kw_id)\n",
    "    doc_set = set()\n",
    "    f = open('{}/{}/inverted-file'.format(curr_dir, model_dir), 'r', encoding='utf-8')\n",
    "    found = False\n",
    "    a = -1\n",
    "    tf = 0\n",
    "    \n",
    "    for i, line in enumerate(f):\n",
    "        if found and i > a:\n",
    "            found = False\n",
    "            a = -1\n",
    "        l = list(str(line).strip().split(' '))\n",
    "        # word id metadata\n",
    "        if len(l) == 3:\n",
    "            if l[0] == kw_id or l[1] == kw_id:\n",
    "                found = True\n",
    "#             if l[1] == '-1':\n",
    "                a = i+int(l[2])\n",
    "                tf += int(l[2])\n",
    "        elif len(l) == 2 and a != -1 and found:\n",
    "            doc_set.add(l[0])\n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "    return (tf, doc_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: keyword id\n",
    "# Output: a tuple of (tf, set of doc_id strings)\n",
    "\n",
    "def get_wid_dict(model_dir):\n",
    "    f = open('{}/{}/inverted-file'.format(curr_dir, model_dir), 'r', encoding='utf-8')\n",
    "    res = {}\n",
    "    curr = None\n",
    "    \n",
    "    for i, line in enumerate(f):\n",
    "        \n",
    "        l = list(str(line).strip().split(' '))\n",
    "        \n",
    "        if len(l) == 3:\n",
    "            curr = int(l[0])\n",
    "            if curr not in res:\n",
    "                res[curr] = {}\n",
    "        elif len(l) == 2:\n",
    "            if int(l[0]) in res[curr]:\n",
    "                res[curr][int(l[0])] += int(l[1])\n",
    "            else:\n",
    "                res[curr][int(l[0])] = int(l[1])\n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 903,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: set of strings of keyword ids\n",
    "# Output: list of sets of doc_id with keyword\n",
    "\n",
    "def get_topic_kw_doc_set(topic_kw_set, model_dir):\n",
    "    res = []\n",
    "    \n",
    "    for kw_id in topic_kw_set:\n",
    "        print(kw_id)\n",
    "        res.append(get_kw_docs_set(kw_id, model_dir))\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics_tf_docs(topics_kw_sets, model_dir):\n",
    "    res = []\n",
    "    \n",
    "    for t in topics_kw_sets:\n",
    "        res.append(get_topic_kw_doc_set(t, model_dir))\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: topics\n",
    "# Output: list of sets of topic keywords\n",
    "\n",
    "def get_topics_keywords(topics):\n",
    "    topics_keywords = []\n",
    "\n",
    "    for t in topics:\n",
    "        kw = get_topic_keywords_set(t)\n",
    "        topics_keywords.append(kw)\n",
    "        \n",
    "    return topics_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: \n",
    "# Output: list of sets of keyword ids for each topic\n",
    "\n",
    "def get_topics_kw_wordid_sets(topics_keywords, model_dir):\n",
    "    topics_kw_sets = []\n",
    "\n",
    "    for kw in topics_keywords:\n",
    "        topics_kw_sets.append(get_keywords_id_set(kw, model_dir))\n",
    "        \n",
    "    return topics_kw_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 907,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_vector_dict(keyword_ids, t_dict, model_dir):\n",
    "    f = open('{}/{}/file-list'.format(curr_dir, model_dir), 'r', encoding='utf-8')\n",
    "    res = {}\n",
    "    \n",
    "    for i, line in enumerate(f):\n",
    "        # find tf : number of times this word occurs in the document i\n",
    "        v = []\n",
    "        for kw_id in list(keyword_ids):\n",
    "            if i in t_dict[kw_id]:\n",
    "                v.append(t_dict[kw_id][i])\n",
    "            else:\n",
    "                v.append(0)\n",
    "    \n",
    "        res[i] = norm_vec(v)\n",
    "        \n",
    "    return res\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 908,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_vectors_all_topics(keyword_id_set, t_dict, model_dir):\n",
    "    res = []\n",
    "    \n",
    "    for s in keyword_id_set:\n",
    "        res.append(get_doc_vector_dict(s, t_dict, model_dir))\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 909,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_vec(vec):\n",
    "    res = []\n",
    "    m = max(vec)\n",
    "    \n",
    "    for v in vec:\n",
    "        if m != 0:\n",
    "            res.append(float(v)/m)\n",
    "        else:\n",
    "            res.append(0.0)\n",
    "            \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 910,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_vectors(kw_lists, idf_dict):\n",
    "    res = []\n",
    "    \n",
    "    for l in kw_lists:\n",
    "        v = []\n",
    "        for kw in l:\n",
    "            v.append(idf_dict[kw])\n",
    "        res.append(v)\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 911,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(qv, dv):\n",
    "    res = []\n",
    "    \n",
    "    for i in range(len(qv)):\n",
    "        tmp = {}\n",
    "        for k, v in dv[i].items():\n",
    "            tmp[k] = np.dot(qv[i], np.array(v))\n",
    "        res.append(tmp)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 912,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_docs(score_vectors, n=5):\n",
    "    res = []\n",
    "    \n",
    "    for s in score_vectors:\n",
    "        res.append(dict(sorted(s.items(), key=lambda item: item[1], reverse=True)[:n]))\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 913,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_name(doc_id, model_dir):\n",
    "    f = '{}/{}/file-list'.format(curr_dir, model_dir)\n",
    "    with open(f) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            if i == int(doc_id):\n",
    "                s = str(l).strip()\n",
    "                s = s.split('/')[3].lower()\n",
    "                return s\n",
    "    return 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 914,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_ids(topics):\n",
    "    res = []\n",
    "    \n",
    "    for t in topics:\n",
    "        title = t[0].text\n",
    "        res.append(title[len(title)-3:])\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 915,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prediction(d, model_dir, ranked_list_name, topics):\n",
    "    res = {}\n",
    "    ids = get_query_ids(topics)\n",
    "    \n",
    "    for i in range(len(d)):\n",
    "        tmp = []\n",
    "        for k, v in d[i].items():\n",
    "            tmp.append(get_doc_name(k, model_dir))\n",
    "        new_key = ids[i]\n",
    "        res[new_key] = \" \".join(tmp)\n",
    "    \n",
    "    res = pd.DataFrame.from_dict(res, orient='index', columns=['retrieved_docs'])\n",
    "    res.index.name = 'query_id'\n",
    "    \n",
    "    res.to_csv('{}/{}.csv'.format(curr_dir, ranked_list_name))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 916,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_N(model_dir):\n",
    "    f = '{}/{}/file-list'.format(curr_dir, model_dir)\n",
    "    with open(f) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 917,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idf(df, N):\n",
    "    return np.log(N/df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 918,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute(t_dict, r, query_file, ranked_list_name, model_dir, NTCIR_dir):\n",
    "    cf_dict = dict([(x, sum(y.values())) for x, y in t_dict.items()])\n",
    "    df_dict = dict([(x, len(y.values())) for x, y in t_dict.items()])\n",
    "    N = get_N(model_dir)\n",
    "    idf_dict = dict([(x, get_idf(df_dict[x], N)) for x, y in t_dict.items()])\n",
    "    \n",
    "    tree = ET.parse('./'+query_file)\n",
    "    topics = get_topics(tree)\n",
    "    \n",
    "    topics_keywords = get_topics_keywords(topics)\n",
    "    topics_kw_sets = get_topics_kw_wordid_sets(topics_keywords, model_dir)\n",
    "    \n",
    "    query_vectors = get_query_vectors(topics_kw_sets, idf_dict)\n",
    "    query_vectors = np.array(query_vectors)\n",
    "\n",
    "    doc_vectors = get_doc_vectors_all_topics(topics_kw_sets, t_dict, model_dir)\n",
    "    score_vectors = get_scores(query_vectors, doc_vectors)\n",
    "    rel_docs_scores = get_n_docs(score_vectors, 20)\n",
    "    \n",
    "    #     ROCCHIO FEEDBACK\n",
    "    if r:\n",
    "        Dr = get_dr(doc_vectors, rel_docs_scores)\n",
    "        Dnr = get_dnr(doc_vectors, rel_docs_scores)\n",
    "        query_vectors = rocchio_feedback(query_vectors, Dr, Dnr)\n",
    "        doc_vectors = get_doc_vectors_all_topics(topics_kw_sets, t_dict, model_dir)\n",
    "        score_vectors = get_scores(query_vectors, doc_vectors)\n",
    "        rel_docs_scores = get_n_docs(score_vectors, 20)\n",
    "    \n",
    "    create_prediction(rel_docs_scores, model_dir, ranked_list_name, topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 919,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dr(doc_vectors, rel_docs_scores):\n",
    "    res = []\n",
    "    \n",
    "    for i in range(len(rel_docs_scores)):\n",
    "        tmp = []\n",
    "        for k, v in rel_docs_scores[i].items():\n",
    "            tmp.append(doc_vectors[i][k])\n",
    "        res.append(tmp)\n",
    "\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 920,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dnr(doc_vectors, rel_docs_scores):\n",
    "    d_c = doc_vectors.copy()\n",
    "    res = []\n",
    "    \n",
    "    for i in range(len(rel_docs_scores)):\n",
    "        tmp = []\n",
    "        for k, v in doc_vectors[i].items():\n",
    "            if k not in rel_docs_scores[i]: \n",
    "                tmp.append(doc_vectors[i][k])\n",
    "        res.append(tmp)\n",
    "\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_queries_v_sum(queries_vectors):\n",
    "    res = []\n",
    "    for qv in queries_vectors:\n",
    "        ar = np.array(qv)\n",
    "        s = np.add.reduce(ar)\n",
    "        res.append(s)\n",
    "        \n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 922,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rocchio_feedback(query_vectors, Dr, Dnr, a=1, b=0.75, g=0.15):\n",
    "    res = []\n",
    "    Dr_sum = get_queries_v_sum(Dr)\n",
    "    Dnr_sum = get_queries_v_sum(Dnr)\n",
    "    query_vectors = np.array(query_vectors)\n",
    "    \n",
    "    for i in range(len(query_vectors)):\n",
    "        qm = a*query_vectors[i] + (b/len(Dr[i]))*Dr_sum[i] - (g/len(Dnr[i]))*Dnr_sum[i]\n",
    "        res.append(qm)\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 923,
   "metadata": {},
   "outputs": [],
   "source": [
    "# argv = ['-r', '-i', '/queries/query-test.xml', '-o', 'ranked_list.csv', '-m', '/model', '-d', '/CIRB010']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 924,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = False\n",
    "query_file = None\n",
    "ranked_list_name = None\n",
    "model_dir = None\n",
    "NTCIR_dir = None\n",
    "\n",
    "for i in range(len(sys.argv)-1):\n",
    "    c = sys.argv[i]\n",
    "    \n",
    "    if c == '-r':\n",
    "        r = True\n",
    "    elif c == '-i':\n",
    "        query_file = sys.argv[i+1]\n",
    "    elif c == '-o':\n",
    "        ranked_list_name = sys.argv[i+1]\n",
    "    elif c == '-m':\n",
    "        model_dir = sys.argv[i+1]\n",
    "    elif c == '-d':\n",
    "        NTCIR_dir = sys.argv[i+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 925,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_id : (doc_id : occurrences)\n",
    "if False:\n",
    "    t_dict = get_wid_dict(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 926,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute(t_dict, r, query_file, ranked_list_name, model_dir, NTCIR_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
